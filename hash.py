# -*- coding: utf-8 -*-
"""Hash.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J3wrQIGO-JZYPvdGqL_nY7daiOwpDfII
"""

import csv
import time
from dataclasses import dataclass
from typing import Callable, Optional, Any, List, Dict

# ---------- Hash table implementation (separate chaining) ----------

@dataclass
class Node:
    key: str
    value: Any
    next: Optional["Node"] = None


class HashTable:
    """
    Simple separate-chaining hash table for string keys.
    Tracks collisions and basic statistics.
    """
    def __init__(self, size: int, hash_func: Callable[[str], int], name: str = ""):
        self.size = size
        self.hash_func = hash_func
        self.name = name or "hash_table"
        self.buckets: List[Optional[Node]] = [None] * size

        # Stats
        self.num_items = 0
        self.collisions = 0

    def _index_for_key(self, key: str) -> int:
        return self.hash_func(key) % self.size

    def insert(self, key: str, value: Any):
        """
        Insert a key/value pair.
        On collision (bucket already used), we append to the chain and increment collisions.
        """
        idx = self._index_for_key(key)
        head = self.buckets[idx]

        if head is None:
            # No collision, just create a new node.
            self.buckets[idx] = Node(key, value)
        else:
            # We count one collision per insertion that hits a non-empty bucket.
            self.collisions += 1
            current = head
            while current.next is not None:
                current = current.next
            current.next = Node(key, value)

        self.num_items += 1

    # ---- Statistics helpers ----

    def bucket_lengths(self) -> List[int]:
        lengths: List[int] = []
        for head in self.buckets:
            length = 0
            current = head
            while current is not None:
                length += 1
                current = current.next
            lengths.append(length)
        return lengths

    def used_buckets(self) -> int:
        return sum(1 for head in self.buckets if head is not None)

    def empty_buckets(self) -> int:
        return self.size - self.used_buckets()

    def longest_chain(self) -> int:
        return max(self.bucket_lengths()) if self.num_items > 0 else 0

    def wasted_space(self) -> int:
        """
        Simple definition from assignment:
        number of unused buckets.
        """
        return self.empty_buckets()


# ---------- Hash functions (5 fundamentally different versions) ----------

def hash_poly_31(s: str) -> int:
    """
    Attempt 1: Classic polynomial rolling hash with base 31.
    """
    h = 0
    for ch in s:
        h = h * 31 + ord(ch)
    return h


def hash_djb2(s: str) -> int:
    """
    Attempt 2: djb2 hash (Dan Bernstein).
    h = 5381; h = h*33 + c
    """
    h = 5381
    for ch in s:
        h = ((h << 5) + h) + ord(ch)  # h*33 + c
    return h & 0xFFFFFFFFFFFFFFFF  # keep it in 64 bits


def hash_fnv1a(s: str) -> int:
    """
    Attempt 3: 64-bit FNV-1a hash.
    """
    FNV_offset_basis = 1469598103934665603
    FNV_prime = 1099511628211

    h = FNV_offset_basis
    for ch in s:
        h ^= ord(ch)
        h *= FNV_prime
    return h & 0xFFFFFFFFFFFFFFFF


def hash_sum_squares(s: str) -> int:
    """
    Attempt 4: Sum of squared character codes, mixed with length.
    Very simple but fundamentally different from the others.
    """
    total = len(s) * 131
    for ch in s:
        c = ord(ch)
        total += c * c + 17 * c
    return total


def hash_two_stage(s: str) -> int:
    """
    Attempt 5: Two-stage hash:
      1) Preprocess string (lowercase, strip surrounding whitespace).
      2) Mix with a polynomial hash using base 53 and an extra XOR step.
    """
    s_norm = s.strip().lower()
    h = 0
    for ch in s_norm:
        h = h * 53 + ord(ch)
        h ^= (h >> 7)
    return h & 0xFFFFFFFFFFFFFFFF


HASH_FUNCTIONS: Dict[str, Callable[[str], int]] = {
    "poly31": hash_poly_31,
    "djb2": hash_djb2,
    "fnv1a": hash_fnv1a,
    "sum_squares": hash_sum_squares,
    "two_stage": hash_two_stage,
}


# ---------- Data loading ----------

def load_movies_from_csv(path: str):
    """
    Reads the CSV file and returns a list of (title, quote) pairs.
    Assumes there are columns named 'movie_title' and 'quote'.
    """
    movies = []
    with open(path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            title = row.get("movie_title", "").strip()
            quote = row.get("quote", "").strip()
            if not title and not quote:
                # Skip rows that don't have either field.
                continue
            movies.append((title, quote))
    return movies


# ---------- Experiment runner ----------

def build_two_tables(
    movies,
    table_size: int,
    hash_func: Callable[[str], int],
    func_name: str
):
    """
    Build:
      - title_table: key = movie title
      - quote_table: key = movie quote
    using the given hash function.

    Returns a dictionary with statistics for each table.
    """
    title_table = HashTable(table_size, hash_func, name=f"title_{func_name}")
    quote_table = HashTable(table_size, hash_func, name=f"quote_{func_name}")

    start = time.perf_counter()
    for title, quote in movies:
        if title:
            title_table.insert(title, {"title": title, "quote": quote})
        if quote:
            quote_table.insert(quote, {"title": title, "quote": quote})
    end = time.perf_counter()
    build_time = end - start

    stats = {
        "hash_name": func_name,
        "table_size": table_size,
        "num_movies": len(movies),
        "build_time_sec": build_time,
        "title": {
            "items": title_table.num_items,
            "collisions": title_table.collisions,
            "wasted_buckets": title_table.wasted_space(),
            "used_buckets": title_table.used_buckets(),
            "longest_chain": title_table.longest_chain(),
        },
        "quote": {
            "items": quote_table.num_items,
            "collisions": quote_table.collisions,
            "wasted_buckets": quote_table.wasted_space(),
            "used_buckets": quote_table.used_buckets(),
            "longest_chain": quote_table.longest_chain(),
        },
    }
    return stats


def print_stats(stats):
    """
    Nicely print the stats dictionary from build_two_tables().
    """
    print("=" * 70)
    print(f"Hash function: {stats['hash_name']}")
    print(f"Table size   : {stats['table_size']}")
    print(f"Num movies   : {stats['num_movies']}")
    print(f"Build time   : {stats['build_time_sec']:.6f} seconds")
    print("--- Title table ---")
    t = stats["title"]
    print(f"  Items          : {t['items']}")
    print(f"  Collisions     : {t['collisions']}")
    print(f"  Used buckets   : {t['used_buckets']}")
    print(f"  Wasted buckets : {t['wasted_buckets']}")
    print(f"  Longest chain  : {t['longest_chain']}")
    print("--- Quote table ---")
    q = stats["quote"]
    print(f"  Items          : {q['items']}")
    print(f"  Collisions     : {q['collisions']}")
    print(f"  Used buckets   : {q['used_buckets']}")
    print(f"  Wasted buckets : {q['wasted_buckets']}")
    print(f"  Longest chain  : {q['longest_chain']}")
    print()


def run_all_attempts(csv_path: str, table_size: int = 1009):
    """
    Convenience function: load data, run all 5 hash functions,
    and print their statistics.

    In your GitHub repo, you can:
      - run them one at a time and commit after each attempt, OR
      - keep this loop and just explain in README which attempt corresponds to which commit.
    """
    movies = load_movies_from_csv(csv_path)
    print(f"Loaded {len(movies)} movies from {csv_path}")
    print()

    all_results = []
    for name, func in HASH_FUNCTIONS.items():
        stats = build_two_tables(movies, table_size=table_size,
                                 hash_func=func, func_name=name)
        print_stats(stats)
        all_results.append(stats)

    return all_results

movies = load_movies_from_csv("MOCK_DATA(1).csv")
stats_poly = build_two_tables(movies, table_size=1009,
                              hash_func=hash_poly_31,
                              func_name="poly31")
print_stats(stats_poly)